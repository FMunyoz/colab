{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "4. Usar modelos Keras con Scikit-learn.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQROpA1GXynh"
      },
      "source": [
        "<h1><font color=\"#113D68\" size=6>Deep Learning con Python y Keras</font></h1>\n",
        "\n",
        "<h1><font color=\"#113D68\" size=5>Parte 3. Multilayer Perceptron</font></h1>\n",
        "\n",
        "<h1><font color=\"#113D68\" size=4>4. Usar modelos Keras con Scikit-learn</font></h1>\n",
        "\n",
        "<br><br>\n",
        "<div style=\"text-align: right\">\n",
        "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqgVa6nlXynj"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"indice\"></a>\n",
        "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
        "\n",
        "* [0. Contexto](#section0)\n",
        "* [1. Introducción](#section1)\n",
        "* [2. Evaluar modelos con validación cruzada](#section2)\n",
        "* [3. Optimización de hiperparámetros](#section3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awnyXwQlXynj"
      },
      "source": [
        "---\n",
        "<a id=\"section0\"></a>\n",
        "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi5ZwDa5Xynk"
      },
      "source": [
        "En esta lección, aprenderemos cómo usar modelos de Deep Learning de Keras con Scikit-learn en Python. Después de completar esta lección, sabrá:\n",
        "* Cómo ajustar un modelo de Keras para usarlo con Scikit-learn.\n",
        "* Cómo evaluar fácilmente los modelos de Keras mediante la validación cruzada en scikit-learn.\n",
        "* Cómo ajustar los hiperparámetros del modelo de Keras mediante la búsqueda GridSearchCV en scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp9ou1lWXynk"
      },
      "source": [
        "import tensorflow as tf\n",
        "# Eliminar warning\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUpX3VUBXynk"
      },
      "source": [
        "---\n",
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKX1BJcqXynl"
      },
      "source": [
        "<a id=\"section1\"></a>\n",
        "# <font color=\"#004D7F\" size=6>1. Introducción</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAhfhP7ZXynl"
      },
      "source": [
        "Scikit-Learn es una librería con todas las funciones de uso general y proporciona muchas utilidades que son útiles en el desarrollo de modelos de Deep Learning. No menos importante:\n",
        "* Evaluación de modelos utilizando métodos de remuestreo como la validación cruzada de k-fold.\n",
        "* Búsqueda y evaluación eficiente de hiperparámetros del modelo.\n",
        "\n",
        "La biblioteca de Keras proporciona un contenedor _(wrapper)_ conveniente para que los modelos de Deep Learning se utilicen como estimadores de clasificación o regresión en scikit-learn. En las siguientes secciones, trabajaremos a través de ejemplos del uso del contenedor `KerasClassifier`. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmu__SMBXynm"
      },
      "source": [
        "---\n",
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4rBPYVMXynm"
      },
      "source": [
        "<a id=\"section2\"></a>\n",
        "# <font color=\"#004D7F\" size=6>2. Evaluar modelos con validación cruzada </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXcoCaERXynm"
      },
      "source": [
        "Las clases `KerasClassifier` y `KerasRegressor` en Keras toman un argumento `build_fn` que es el nombre de la función a llamar para crear su modelo. Debes definir una función llamada lo que quieras que defina tu modelo, lo compile y lo devuelva. En el siguiente ejemplo, \n",
        "\n",
        "1. Definimos una función `create_model()` que crea una red neuronal MLP.\n",
        "2. Pasamos este nombre de función a la clase `KerasClassifier` mediante el argumento `build_fn`.\n",
        "3. También pasamos argumentos adicionales de `epochs = 150` y `batch_size = 10`. Estos se agrupan automáticamente y se pasan a la función `fit()` que es llamada internamente por la clase `KerasClassifier`. \n",
        "4. Utilizamos la clase `StratifiedKFold` de Scikit-Learn con 10-fold. \n",
        "5. Usamos la función scikit-learn `cross_val_score()` para evaluar nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnma3Z7VXynm",
        "outputId": "d7db5769-5a97-4296-d8c3-dbe5aaf1ed77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(12, input_dim=8, activation = 'relu'))\n",
        "  model.add(Dense(8, activation = 'relu'))\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\n",
        "  # compile\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = np.loadtxt('https://raw.githubusercontent.com/FMunyoz/colab/main/Datasets/pima-indians-diabetes.csv', delimiter=',')\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:, 0:8]\n",
        "y = dataset[:, 8]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
        "# evaluate using 10-fold cross validation\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "resulsts = cross_val_score(model, X, y, cv=kfold)\n",
        "print(resulsts.mean())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6926691710948945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APOL0FRYXynn"
      },
      "source": [
        "Puede ver que cuando se ajusta el modelo de Keras, la estimación de la precisión del modelo puede simplificarse en gran medida, en comparación con la enumeración manual de validación cruzada realizada en la lección anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4cW1xVXXyno"
      },
      "source": [
        "---\n",
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtyBGdSBXyno"
      },
      "source": [
        "<a id=\"section3\"></a>\n",
        "# <font color=\"#004D7F\" size=6>3. Optimización de hiperparámetros</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ZJdM1JXyno"
      },
      "source": [
        "En este ejemplo, usamos `GridSearchCV` para evaluar diferentes configuraciones para nuestro modelo de red neuronal e informar sobre la combinación que proporciona el mejor rendimiento estimado. \n",
        "\n",
        "La función `create_model()` está definida para tomar dos argumentos `optimizer` e `init`, los cuales deben tener valores predeterminados. Específicamente, los hiperparámetros a establecer serán:\n",
        "* Optimizadores para establecer los pesos.\n",
        "* Inicializadores para los pesos.\n",
        "* Número de épocas para entrenar el modelo.\n",
        "* Batchs para variar el número de muestras antes de las actualizaciones de peso.\n",
        "\n",
        "Las opciones se especifican en un diccionario y se pasan a la configuración de la clase `GridSearchCV`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SXxnfCU4Xyno",
        "outputId": "674d12b5-06e3-45cc-930f-abaa3ecb26c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop',init='glorot_uniform'):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(12, input_dim=8, kernel_initializer=init, activation = 'relu'))\n",
        "  model.add(Dense(8, kernel_initializer=init, activation = 'relu'))\n",
        "  model.add(Dense(1, kernel_initializer=init, activation = 'sigmoid'))\n",
        "  # compile\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = np.loadtxt('https://raw.githubusercontent.com/FMunyoz/colab/main/Datasets/pima-indians-diabetes.csv', delimiter=',')\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:, 0:8]\n",
        "y = dataset[:, 8]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "\n",
        "optimizers = ['rmsprop', 'adam']\n",
        "inits = ['glorot_uniform','normal', 'uniform']\n",
        "epochs = [50,100,150]\n",
        "batches = [5, 10, 20]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
        "grid = GridSearchCV(estimator=model, param_grid= param_grid, cv=3)\n",
        "grid_result = grid.fit(X, y)\n",
        "# summarize results\n",
        "print(\"Mejor: %f usando %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) con: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mejor: 0.730469 usando {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.546875 (0.119636) con: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.658854 (0.025780) con: {'batch_size': 5, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.688802 (0.035277) con: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.723958 (0.030978) con: {'batch_size': 5, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.712240 (0.033197) con: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.699219 (0.014616) con: {'batch_size': 5, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.700521 (0.033502) con: {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.688802 (0.007366) con: {'batch_size': 5, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.722656 (0.008438) con: {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.712240 (0.010253) con: {'batch_size': 5, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.665365 (0.051855) con: {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.723958 (0.023939) con: {'batch_size': 5, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.662760 (0.022628) con: {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.695312 (0.055883) con: {'batch_size': 5, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.720052 (0.015733) con: {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.726562 (0.003189) con: {'batch_size': 5, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.729167 (0.033804) con: {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.704427 (0.030145) con: {'batch_size': 5, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.610677 (0.041626) con: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.657552 (0.036272) con: {'batch_size': 10, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.683594 (0.030425) con: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.696615 (0.024150) con: {'batch_size': 10, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.690104 (0.031948) con: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.687500 (0.003189) con: {'batch_size': 10, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.678385 (0.007366) con: {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.666667 (0.036272) con: {'batch_size': 10, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.686198 (0.023510) con: {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.714844 (0.012758) con: {'batch_size': 10, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.699219 (0.016877) con: {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.692708 (0.026748) con: {'batch_size': 10, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.653646 (0.014731) con: {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.688802 (0.036690) con: {'batch_size': 10, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.730469 (0.016877) con: {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.703125 (0.011500) con: {'batch_size': 10, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.701823 (0.034104) con: {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.709635 (0.016367) con: {'batch_size': 10, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.628906 (0.017758) con: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.632812 (0.045999) con: {'batch_size': 20, 'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.695312 (0.019918) con: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.667969 (0.041463) con: {'batch_size': 20, 'epochs': 50, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.674479 (0.012890) con: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.665365 (0.034987) con: {'batch_size': 20, 'epochs': 50, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.611979 (0.064607) con: {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.670573 (0.008027) con: {'batch_size': 20, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.712240 (0.017566) con: {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.707031 (0.005524) con: {'batch_size': 20, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.703125 (0.024910) con: {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.716146 (0.013279) con: {'batch_size': 20, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.680990 (0.041626) con: {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
            "0.686198 (0.004872) con: {'batch_size': 20, 'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.710938 (0.014616) con: {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop'}\n",
            "0.701823 (0.019225) con: {'batch_size': 20, 'epochs': 150, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.682292 (0.036966) con: {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop'}\n",
            "0.727865 (0.024150) con: {'batch_size': 20, 'epochs': 150, 'init': 'uniform', 'optimizer': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBLhpVJ-Xynp"
      },
      "source": [
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---\n",
        "\n",
        "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
      ]
    }
  ]
}